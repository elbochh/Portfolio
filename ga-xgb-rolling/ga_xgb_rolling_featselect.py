# -*- coding: utf-8 -*-
"""ga_xgb_rolling_featselect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QhJzjispc6cuEJTrPM4N4PKWnF5f5s98
"""

import os
import json
import math
import random
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
import xgboost as xgb
from pathlib import Path

# ----------------------------- CONFIG -----------------------------
PARQUET_PATH  = "features_lgbm_final.parquet"   # input
TARGET_COL    = "y_ret_next"
EXCLUDE_COLS  = {"target_updown", "date", "ticker"}  # always exclude from features

TOP_K_FEATURES = 20

# GA hyperparams
POP_SIZE      = 75
N_GEN         = 5
ELITE_K       = 7
TOURN_SIZE    = 2
MUT_PROB_FEAT = 0.04
MUT_PROB_PARAM= 0.3
SEED          = 42

# XGBoost training params
NUM_BOOST_ROUND   = 3000


# Safety minimums
MIN_ROWS_TRAIN = 2000
MIN_ROWS_VAL   = 500
MIN_ROWS_TEST  = 500

# ----------------------------- UTILS -----------------------------
def set_seeds(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)

def direction_win_rate(y_true, y_pred, thr: float = 0.0, exclude_ties: bool = False) -> float:
    yt = np.asarray(y_true, dtype=float)
    yp = np.asarray(y_pred, dtype=float)
    sign_true = np.where(yt > thr, 1, np.where(yt < -thr, -1, 0))
    sign_pred = np.where(yp > thr, 1, np.where(yp < -thr, -1, 0))
    if exclude_ties:
        mask = (sign_true != 0) & (sign_pred != 0)
        if mask.sum() == 0:
            return 0.0
        wins = (sign_true[mask] == sign_pred[mask]).mean()
    else:
        wins = (sign_true == sign_pred).mean()
    return float(wins * 100.0)

def rmse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred, squared=False)

def safe_numeric_cols(df: pd.DataFrame, exclude: set, target: str) -> List[str]:
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    to_drop = set([target]) | set(exclude)
    return [c for c in num_cols if c not in to_drop]

def unique_date_quantile_cutoff(dates: pd.Series, perc: float) -> pd.Timestamp:
    unique = np.sort(dates.dropna().unique())
    idx = int(np.floor(perc * len(unique)))
    idx = max(0, min(idx, len(unique) - 1))
    return pd.Timestamp(unique[idx])

def print_split_sizes(tag: str, tr: pd.DataFrame, va: pd.DataFrame, te: pd.DataFrame):
    print(f"[{tag}] sizes | train:{len(tr):,}  val:{len(va):,}  test:{len(te):,}")

# ------------------------- PARAM SPACES ---------------------------
def random_xgb_params() -> Dict:
    return {
        "objective": "reg:squarederror",
        "tree_method": "hist",
        "eval_metric": "rmse",
        "max_depth": int(np.random.randint(3, 12)),
        "eta": float(np.random.uniform(0.01, 0.2)),
        "subsample": float(np.random.uniform(0.6, 1.0)),
        "colsample_bytree": float(np.random.uniform(0.6, 1.0)),
        "min_child_weight": int(np.random.randint(1, 20)),
        "lambda": float(np.random.uniform(0.0, 5.0)),
        "alpha": float(np.random.uniform(0.0, 5.0)),
        "seed": SEED,
    }

def mutate_params(p: Dict) -> Dict:
    q = p.copy()
    if np.random.rand() < MUT_PROB_PARAM:
        q["max_depth"] = int(np.clip(q["max_depth"] + np.random.randint(-2, 3), 2, 15))
    if np.random.rand() < MUT_PROB_PARAM:
        q["eta"] = float(np.clip(q["eta"] * np.exp(np.random.uniform(-0.3, 0.3)), 0.001, 0.3))
    if np.random.rand() < MUT_PROB_PARAM:
        q["subsample"] = float(np.clip(q["subsample"] + np.random.uniform(-0.1, 0.1), 0.4, 1.0))
    if np.random.rand() < MUT_PROB_PARAM:
        q["colsample_bytree"] = float(np.clip(q["colsample_bytree"] + np.random.uniform(-0.1, 0.1), 0.4, 1.0))
    if np.random.rand() < MUT_PROB_PARAM:
        q["min_child_weight"] = int(np.clip(q["min_child_weight"] + np.random.randint(-2, 3), 1, 50))
    if np.random.rand() < MUT_PROB_PARAM:
        q["lambda"] = float(np.clip(q["lambda"] + np.random.uniform(-0.5, 0.5), 0.0, 10.0))
    if np.random.rand() < MUT_PROB_PARAM:
        q["alpha"] = float(np.clip(q["alpha"] + np.random.uniform(-0.5, 0.5), 0.0, 10.0))
    return q

def crossover_params(p1: Dict, p2: Dict) -> Dict:
    child = {}
    for k in p1:
        child[k] = p1[k] if np.random.rand() < 0.5 else p2[k]
    return child

# ---------------------- FEATURE GENOME OPS ------------------------
def random_mask(n_feats: int, k: int) -> np.ndarray:
    idx = np.random.choice(n_feats, size=k, replace=False)
    mask = np.zeros(n_feats, dtype=bool)
    mask[idx] = True
    return mask

def repair_mask(mask: np.ndarray, k: int) -> np.ndarray:
    m = mask.copy()
    c = int(m.sum())
    if c > k:
        drop = np.random.choice(np.where(m)[0], size=c - k, replace=False)
        m[drop] = False
    elif c < k:
        add = np.random.choice(np.where(~m)[0], size=k - c, replace=False)
        m[add] = True
    return m

def mutate_mask(mask: np.ndarray, k: int, p: float) -> np.ndarray:
    m = mask.copy()
    flips = np.random.rand(len(m)) < p
    m[flips] = ~m[flips]
    return repair_mask(m, k)

def crossover_masks(m1: np.ndarray, m2: np.ndarray, k: int) -> np.ndarray:
    child = np.where(np.random.rand(len(m1)) < 0.5, m1, m2)
    return repair_mask(child, k)

# --------------------------- GA TYPES -----------------------------
GLOBAL_INDIV_ID = 0

@dataclass
class Individual:
    id: int
    gen: int
    mask: np.ndarray
    params: Dict
    fitness: float = math.inf

# ------------------------- EVALUATION -----------------------------
def fit_eval_rmse(params: Dict, X_tr: pd.DataFrame, y_tr: pd.Series,
                  X_va: pd.DataFrame, y_va: pd.Series) -> Tuple[xgb.XGBRegressor, np.ndarray, float]:
    model = xgb.XGBRegressor(**params, n_estimators=NUM_BOOST_ROUND)
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_va, y_va)],


        verbose=False
    )
    preds = model.predict(X_va)
    val_rmse = rmse(y_va, preds)
    return model, preds, val_rmse

def log_and_print_individual(ind: Individual, feat_names: List[str],
                             X_tr: pd.DataFrame, y_tr: pd.Series,
                             X_va: pd.DataFrame, y_va: pd.Series,
                             window_tag: str):
    cols = [feat_names[i] for i, b in enumerate(ind.mask) if b]
    _, preds, fit_rmse = fit_eval_rmse(ind.params, X_tr[cols], y_tr, X_va[cols], y_va)
    ind.fitness = fit_rmse
    win = direction_win_rate(y_va.values, preds, thr=0.0)

    p = ind.params
    print(
        f"[{window_tag}] Gen {ind.gen:02d} | ID {ind.id:05d} "
        f"| RMSE={fit_rmse:.6f} | WIN={win:.2f}% | feats={len(cols)} "
        f"| depth={p['max_depth']} lr={p['eta']:.4f} "
        f"sub={p['subsample']:.2f} col={p['colsample_bytree']:.2f} "
        f"mcw={p['min_child_weight']} l1={p['alpha']:.3f} l2={p['lambda']:.3f}"
    )

# ----------------------- DATA / WINDOWS ---------------------------
def build_windows_by_percentiles() -> List[Tuple[float, float]]:
    return [(0.50, 0.60), (0.60, 0.70), (0.70, 0.80), (0.80, 0.90), (0.90, 1.00)]

def slice_splits(df: pd.DataFrame, train_end_pct: float, test_end_pct: float):
    udates = np.sort(df["date"].dropna().unique())
    n = len(udates)

    def cut(p):
        idx = int(np.floor(p * n))
        idx = max(0, min(idx, n - 1))
        return pd.Timestamp(udates[idx])

    p_int_val_start = train_end_pct - 0.10
    dt0 = cut(0.0)
    dt_tr_end = cut(p_int_val_start)
    dt_val_end = cut(train_end_pct)
    dt_test_end = cut(test_end_pct)

    internal_train = df[(df["date"] >= dt0) & (df["date"] < dt_tr_end)]
    internal_val   = df[(df["date"] >= dt_tr_end) & (df["date"] < dt_val_end)]
    test_df        = df[(df["date"] >= dt_val_end) & (df["date"] < dt_test_end)]
    return internal_train, internal_val, test_df

# --------------------------- MAIN LOOP ----------------------------
def main(df):
    set_seeds(SEED)
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date", TARGET_COL]).sort_values(["date"]).reset_index(drop=True)

    feat_names = safe_numeric_cols(df, EXCLUDE_COLS, TARGET_COL)
    if len(feat_names) < TOP_K_FEATURES:
        raise ValueError(f"Need at least {TOP_K_FEATURES} numeric features; got {len(feat_names)}.")

    windows = build_windows_by_percentiles()
    global GLOBAL_INDIV_ID
    GLOBAL_INDIV_ID = 0

    for (tr_end_pct, te_end_pct) in windows:
        window_tag = f"{int(tr_end_pct*100)}_{int(te_end_pct*100)}"
        tr, va, te = slice_splits(df, tr_end_pct, te_end_pct)

        if len(tr) < MIN_ROWS_TRAIN or len(va) < MIN_ROWS_VAL or len(te) < MIN_ROWS_TEST:
            print(f"[{window_tag}] Skipping window due to insufficient rows.")
            continue

        print_split_sizes(window_tag, tr, va, te)
        X_tr, y_tr = tr[feat_names], tr[TARGET_COL]
        X_va, y_va = va[feat_names], va[TARGET_COL]

        pop: List[Individual] = []
        for _ in range(POP_SIZE):
            mask = random_mask(len(feat_names), TOP_K_FEATURES)
            params = random_xgb_params()
            GLOBAL_INDIV_ID += 1
            pop.append(Individual(id=GLOBAL_INDIV_ID, gen=0, mask=mask, params=params, fitness=math.inf))

        print(f"[{window_tag}] === Begin GA ===")
        for ind in pop:
            if not np.isfinite(ind.fitness):
                log_and_print_individual(ind, feat_names, X_tr, y_tr, X_va, y_va, window_tag)

        for g in range(1, N_GEN + 1):
            pop.sort(key=lambda ind: ind.fitness)
            next_gen: List[Individual] = []
            for i in range(min(ELITE_K, len(pop))):
                elite = pop[i]
                next_gen.append(Individual(
                    id=elite.id, gen=g, mask=elite.mask.copy(),
                    params=elite.params.copy(), fitness=elite.fitness
                ))

            while len(next_gen) < POP_SIZE:
                p1 = random.choice(pop[:max(ELITE_K, TOURN_SIZE)])
                p2 = random.choice(pop[:max(ELITE_K, TOURN_SIZE)])
                child_mask = crossover_masks(p1.mask, p2.mask, TOP_K_FEATURES)
                child_mask = mutate_mask(child_mask, TOP_K_FEATURES, MUT_PROB_FEAT)
                child_params = crossover_params(p1.params, p2.params)
                child_params = mutate_params(child_params)
                GLOBAL_INDIV_ID += 1
                next_gen.append(Individual(id=GLOBAL_INDIV_ID, gen=g, mask=child_mask, params=child_params, fitness=math.inf))

            for ind in next_gen:
                if not np.isfinite(ind.fitness):
                    log_and_print_individual(ind, feat_names, X_tr, y_tr, X_va, y_va, window_tag)

            pop = next_gen
            best = min(pop, key=lambda ind: ind.fitness)
            print(f"[{window_tag}] Gen {g:02d} COMPLETE | best RMSE={best.fitness:.6f}")

        # ---- Final test ----
        best = min(pop, key=lambda ind: ind.fitness)
        X_final = pd.concat([X_tr, X_va])
        y_final = pd.concat([y_tr, y_va])
        cols = [feat_names[i] for i, b in enumerate(best.mask) if b]
        model_best = xgb.XGBRegressor(**best.params, n_estimators=NUM_BOOST_ROUND)
        model_best.fit(X_final[cols], y_final, verbose=False)
        preds_test = model_best.predict(te[cols])
        test_rmse = rmse(te[TARGET_COL], preds_test)
        test_win = direction_win_rate(te[TARGET_COL].values, preds_test, thr=0.0)
        print(f"[{window_tag}] TEST RMSE={test_rmse:.6f} | TEST WIN={test_win:.2f}%")

# --------------- ENTRY POINT ---------------
if __name__ == "__main__":
    parquet_path = Path(PARQUET_PATH)
    if not parquet_path.exists():
        raise FileNotFoundError(f"Parquet file not found: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    main(df)

